---
title: "Experiment 3B"
author: "Hongmi Lee, Kyungmi Kim, Do-Joon Yi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: 
      collapse: false
      smooth_scroll: false
    number_sections: true
    theme: simplex 
    highlight: tango
    code_folding: show
subtitle: Two-day experiment with pseudowords
mainfont: Noto Sans CJK KR
---

```{r, echo=FALSE}
setwd("~/Dropbox/2017Experiment/newOSF/")
options(knitr.kable.NA = '')
```


# Pre-experimental Familiarization

Thirteen participants were pre-familiarized with 48 `pseudowords`. Participants completed a total of 1,440 trials, divided into 10 blocks of 144 trials each. Each item was repeated three times within a block. Thus, participants were exposed to each item 30 times in total. In each trial, participants indicated whether it was the first, second, or third time the given item was presented within the block. 

To analyze the data, some packages need to be loaded. We use `pacman` as a package manager, which takes care of the other packages.
```{r}
if (!require("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(tidyverse, knitr, beeswarm, car, afex, emmeans, cowplot)
pacman::p_load_gh("crsh/papaja")
```

The sum-to-zero coding scheme was used whenever `(g)lmer` was invoked or the type III tests were calculated ([Singmann & Kellen, in press](http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf)).
```{r, eval=TRUE, message=FALSE}
afex::set_sum_contrasts()
```

## Data
```{r}
EE <- read.csv("data/data_NovSM_Exp3Word_PRE.csv", header = T)
glimpse(EE, width=70)
```
1. SID: participant ID
2. Block: 1~10
3. Trial: 1~144
4. RepTime: 3 repetitions per image per block. 1~3
5. Resp: repetition counting, 1~3, 0 = no response
6. Corr: Correctness, 1=correct, 0=incorrect or no response
7. RT: reaction times in ms.
8. ImgName: name of stimuli

Two participants (5th & 6th) did not complete the session due to a technical issue.
```{r}
table(EE$SID)
```


## Accuracy & RT

```{r}
data <- EE %>% group_by(SID, Block) %>% 
  summarise(Accuracy = mean(Corr)) %>% 
  ungroup()
h1 <- ggplot(data, aes(x=Block, y=Accuracy, group=SID)) + 
  geom_line() +
  scale_x_continuous(breaks=seq(1,10,1)) +
  # scale_y_continuous(limits=c(0, 1)) +
  theme_apa()

data <- EE %>% filter(Corr == 1) %>% 
  group_by(SID, Block) %>% 
  summarise(RT = mean(RT)) %>% 
  ungroup()
h2 <- ggplot(data, aes(x=Block, y=RT, group=SID)) + 
  geom_line() +
  scale_x_continuous(breaks=seq(1,10,1)) + 
  expand_limits(y=0) +
  labs(y = "Correct RT") +
  theme_apa()

plot_grid(h1, h2)
```

Two participants (6th & 7th) did not make responses in all or most of trials in Block 1. In subsequent blocks, participants made faster correct responses while maintaining a similar level of accuracy. Overall accuracy was `r round(EE %>% filter(Block > 1) %>% summarise(Average = mean(Corr)), digits=4)*100`%. 


# Item Repetition Phase

Twenty four pseudowords were repeated eight times, producing 192 trials. In each trial, participants made a preference judgment for an item.

## Data

```{r}
RR <- read.csv("data/data_NovSM_Exp3Word_REP.csv", header = T)
RR$Pref <- ifelse(RR$Resp==1,1,0)

glimpse(RR, width=70)
```
1. SID: participant ID
2. RepTime: number of repetition, 1~8
3. Trial: 1~24
4. Resp: preference judgment, 1 = like, 2 = dislike, 3 = no response
5. Corr: Correctness, 1=response, 0 = no response
6. RT: reaction times in ms.
7. ImgName: name of stimuli
8. Pref: preference, 0=dislike (treating no response as 'dislike'), 1=like

```{r}
table(RR$SID)
```

Participants did not make responses in `r round(100 - mean(RR$Corr)*100, digits=2)`% of trials.
```{r}
1 - mean(RR$Corr)
```

## Preference

```{r, fig.asp=.5}
data <- RR %>% group_by(SID) %>% 
  summarise(Preference = mean(Pref)) %>% 
  ungroup

boxplot(data$Preference, horizontal = TRUE, 
        ylim = c(0,1), boxwex = .8, boxcol = "grey", 
        xlab = "Preference Judgment (0 = dislike, 1 = like)")
beeswarm(data$Preference, horizontal = TRUE, add = TRUE,
         col = alpha("blue", 0.5), pch=16)
```

The mean preference was `r round(mean(data$Preference)*100, digits=2)`%.

****

# Item-Source Association Phase

Participants learned 48 pairs of a pseudoword and a quadrant on the screen. Half of pseudowords were those shown in the first phase. The other half were new. Participants were instructed to pay attention to the location of each pseudowords while making a preference judgment.

## Data
```{r}
SS <- read.csv("data/data_NovSM_Exp3Word_SRC.csv", header = T)
SS$Repetition = factor(SS$Repetition, levels=c(1,2), labels=c("Repetition","No Repetition"))
SS$Pref <- ifelse(SS$Resp==1,1,0)

glimpse(SS, width=70)
```
1. SID: participant ID
2. Trial: 1~48
3. Repetition: 1 = repetition, 2 = no repetition
4. Loc: location (source) of memory item; quadrants, 1~4
5. Resp: preference judgment, 1 = like, 2 = dislike, 3 = no response
6. Corr: Correctness, 1=response, 0 = no response
7. RT: reaction times in ms.
8. ImgName: name of stimuli
9. Pref: preference, 0=dislike (treating no response as 'dislike'), 1=like

The within-participant **item repetition** was the single factor in Experiment 3B. Participants had 24 `repetition` trials and 24 `no repetition` trials.
```{r}
table(SS$Repetition, SS$SID)
```

Participants did not make responses in `r round(100 - mean(SS$Corr)*100, digits=2)`% of trials.
```{r}
1 - mean(SS$Corr)
```


## Preference

As in Experiment 2, preference for `pseudowords` increased after `repetition`.

```{r}
apa_beeplot(data = SS,
            id="SID", dv="Pref", factors=c("Repetition"), 
            dispersion = within_subjects_conf_int,
            ylim = c(0, 1),
            xlab = "Item Repetition",
            ylab = "Preference Judgment (0 = dislike, 1 = like)",
            las=1)

SS %>% group_by(SID, Repetition) %>% 
  summarise(Preference = mean(Pref)) %>% 
  ungroup %>% 
  group_by(Repetition) %>%
  summarise(meanAcc = mean(Preference), ciAcc = ci(Preference)) %>% 
  ungroup %>% 
  kable()
```


### ANOVA

```{r, warning=FALSE}
ss.aov <- aov_ez(id = "SID", dv = "Pref", data = SS, within = "Repetition")
anova(ss.aov, es = "pes") %>% kable(digits = 4)
```

Participants preferred the pseudowords of the `repetition` condition significantly to those of the `no repetition` condition (*familiarity benefit*). 

```{r, eval=FALSE, echo=FALSE}
( ss.aov.emm <- emmeans(ss.aov, pairwise ~ Repetition) )
plot(ss.aov.emm, comparison = TRUE)
```

### GLMM

The full model (`ss.m1`) included a fixed effect (**item repetition**). The model was maximal in that it included both by-participant and by-item random intercepts, and by-participant random slopes for **item repetition** ([Barr, Levy, Scheepers, & Tily, 2013](https://www.sciencedirect.com/science/article/pii/S0749596X12001180)).

To fit the models, we used the `mixed()` of the `afex` package ([Singmann, Bolker, & Westfall, 2017](https://afex.singmann.science/forums/topic/citing-afex)) which was built on the `lmer()` of the `lme4` package ([Bates, Maechler, Bolker, & Walker, 2015](https://www.jstatsoft.org/article/view/v067i01/0)). The `mixed()` assessed the statistical significance of fixed effects by comparing a model with the effect in question against its nested model which lacked the effect in question. *P*-values of the effects were obtained by both likelihood ratio tests.

```{r, eval=FALSE}
pacman::p_load(parallel)
(nc <- detectCores())
cl <- makeCluster(rep("localhost", nc))

ss.m1 <- mixed(Pref ~ Repetition + (Repetition|SID) + (1|ImgName), 
               SS, method = "LRT", cl = cl, 
               family=binomial(link="logit"),
               control = glmerControl(optCtrl = list(maxfun = 1e6)))

stopCluster(cl)
```

```{r, echo=FALSE}
load('RData/afex_NovSM_Exp3Word_Pref.RData')
```

```{r}
kable(anova(ss.m1))
```

The result is consistent with those from the ANOVA.

```{r, eval=FALSE, echo=FALSE}
( ss.full.emm <- emmeans(ss.m1, pairwise ~ Repetition, type = "response") )
plot(ss.full.emm, comparison = TRUE)
```


****

# Source Memory Test Phase

In each trial, participants first indicated in which quadrant a given pseudoword appeared during the item-source association phase. Participants then rated how confident they were about their memory judgment. 

## Data

```{r}
WW <- read.csv("data/data_NovSM_Exp3Word_TST.csv", header = T)
WW$Repetition = factor(WW$Repetition, levels=c(1,2), labels=c("Repetition","No Repetition"))

glimpse(WW, width=70)
```
1. SID: participant ID
2. Trial: 1~48
3. Repetition: 1 = repetition, 2 = no repetition
4. AscLoc: location (source) in which the item was presented in Phase 2; quadrants, 1~4
5. SrcResp: source response; quadrants, 1~4
6. Corr: Correctness, 1=correct ,2=incorrect & no response
7. RT: reaction times in ms.
8. Confident: confidence rating, 1~4
9. ImgName: name of stimuli

The design was the same as that in the second phase. A single factor was the within-participant **item repetition** (`repetition` vs. `no repetition`).

```{r}
table(WW$Repetition, WW$SID)
```

## Accuracy

```{r}
apa_beeplot(data = WW, 
            id="SID", dv="Corr", factors="Repetition", 
            dispersion = within_subjects_conf_int,
            ylim = c(0, 1),
            xlab = "Item Repetition",
            ylab = "Source Memory Accuracy",
            las=1)

WW %>% group_by(SID, Repetition) %>% 
  summarise(Accuracy = mean(Corr)) %>% 
  ungroup %>% 
  group_by(Repetition) %>%
  summarise(mean.Accuracy = mean(Accuracy), ci.Accuracy = ci(Accuracy)) %>% 
  ungroup %>% 
  kable()
```

Source memory accuracy was greater in the `no repetition` condition than the `repetition` condition (*novelty benefit*). 


### ANOVA

```{r, warning=FALSE}
ww.corr.aov <- aov_ez(id = "SID", dv = "Corr", data = WW, within = "Repetition")
anova(ww.corr.aov, es = "pes") %>% kable(digits = 4)
```

The source of unrepeated pseudowords was significantly better remembered than that of repeated pseudowords. 


```{r, eval=FALSE, echo=FALSE}
( ww.corr.aov.emm <- emmeans(ww.corr.aov, pairwise ~ Repetition) )
plot(ww.corr.aov.emm, comparison = TRUE)
```


### GLMM

We built the full model (`full1`) with a fixed effect (**item repetition**). The model was maximal in that it included both by-participant and by-item random intercepts, and by-participant random slopes for **item repetition** ([Barr, Levy, Scheepers, & Tily, 2013](https://www.sciencedirect.com/science/article/pii/S0749596X12001180)). In case the maximal model does not converge successfully, we built another model (`full2`) with the maximal random structure but with the correlations among the random terms removed ([Singmann, 2018](https://cran.r-project.org/web/packages/afex/vignettes/afex_mixed_example.html#lrt-results)). *P*-values of the effects were obtained by both likelihood ratio tests and parametric bootstrap tests with 1,000 simulations.

```{r, eval=FALSE}
pacman::p_load(parallel, optimx)
(nc <- detectCores())
cl <- makeCluster(rep("localhost", nc))

full1 <- mixed(Corr ~ Repetition + (Repetition|SID) + (1|ImgName), 
               WW, method = "LRT", cl = cl, 
               family=binomial(link="logit"),
               control = glmerControl(optCtrl = list(maxfun = 1e6)))
full2 <- mixed(Corr ~ Repetition + (Repetition||SID) + (1|ImgName),
               WW, method = "LRT", cl = cl, 
               family=binomial(link="logit"),
               control = glmerControl(optCtrl = list(maxfun = 1e6)), expand_re = TRUE)

clusterEvalQ(cl, library(optimx))

full1pb <- mixed(Corr ~ Repetition + (Repetition|SID) + (1|ImgName), 
               WW, method = "PB", args_test = list(nsim = 1000, cl = cl), 
               cl = cl, family=binomial(link="logit"),
               control = glmerControl(optCtrl = list(maxfun = 1e6)))
full2pb <- mixed(Corr ~ Repetition + (Repetition||SID) + (1|ImgName),
               WW, method = "PB", args_test = list(nsim = 1000, cl = cl), 
               cl = cl, family=binomial(link="logit"),
               control = glmerControl(optCtrl = list(maxfun = 1e6)), expand_re = TRUE)

stopCluster(cl)
```

```{r, echo=FALSE}
load("RData/afex_NovSM_Exp3Word_Acc_all.RData")
```

Here are the LRT results.
```{r}
full.compare <- cbind(afex::nice(full1), afex::nice(full2)[,-c(1,2)])
colnames(full.compare)[c(3,4,5,6)] <- paste0(rep(c("full1_", "full2_"), each = 2), 
                                             colnames(full.compare)[c(3,4)])
full.compare %>% kable()
```

Here are the parametric bootstrap test results.
```{r}
full.compare <- cbind(afex::nice(full1pb), afex::nice(full2pb)[,-c(1,2)])
colnames(full.compare)[c(3,4,5,6)] <- paste0(rep(c("full1pb_", "full2pb_"), each = 2), 
                                             colnames(full.compare)[c(3,4)])
full.compare %>% kable()
```

The *p*-values from all models agreed strongly providing a high degree of confidence in the results. 

```{r, eval=FALSE, echo=FALSE}
( ww.full1.emm <- emmeans(full1, pairwise ~ Repetition, type = "response") )
plot(ww.full1.emm, comparison = TRUE)
```



## Confidence

There was only negligible difference in confidence between the `repetition` vs. `no repetition` conditions. 

```{r}
apa_beeplot(data = WW, 
            id="SID", dv="Confident", factors="Repetition", 
            dispersion = within_subjects_conf_int,
            ylim = c(1, 4),
            xlab = "Item Repetition",
            ylab = "Source Memory Confidence",
            las=1)

WW %>% group_by(SID, Repetition) %>% 
  summarise(Confidence = mean(Confident)) %>% 
  ungroup %>% 
  group_by(Repetition) %>%
  summarise(mean.Confidence = mean(Confidence), ci.Confidence = ci(Confidence)) %>% 
  ungroup %>% 
  kable()
```

### ANOVA

```{r, warning=FALSE}
ww.conf.aov <- aov_ez(id = "SID", dv = "Confident", data = WW, within = "Repetition")
anova(ww.conf.aov, es = "pes") %>% kable(digits = 4)
```

```{r, eval=FALSE, echo=FALSE}
( ww.conf.aov.emm <- emmeans(ww.conf.aov, pairwise ~ Repetition) )
plot(ww.conf.aov.emm, comparison = TRUE)
```

The effect of `item repetition` was not significant in confidence ratings.

### CLMM

The responses from a Likert-type scale are ordinal. Especially for the rating items with numerical response formats containing four or fewer categories, it is recommended to use categorical data analysis approaches, rather than treating the responses as continuous data ([Harpe, 2015](https://ac.els-cdn.com/S1877129715200196/1-s2.0-S1877129715200196-main.pdf?_tid=589e7c17-4d55-49e6-a80f-5aef938a1f44&acdnat=1549722329_bb934739b91ee6554cafadb39565dc25)).

Here we employed the cumulative link mixed modeling using the `clmm()` of the package `ordinal` ([Christensen, submitted](https://cran.r-project.org/web/packages/ordinal/vignettes/clm_article.pdf)). The model specification of fixed and random effects was the same as the `mixed()` above.
```{r, eval=FALSE}
pacman::p_load(ordinal)

WW2 <- WW
WW2$Confident = factor(WW2$Confident, ordered = TRUE)
WW2$SID = factor(WW2$SID)

cm.full <- clmm(Confident ~ Repetition + (Repetition|SID) + (1|ImgName), data=WW2)
cm.red1 <- clmm(Confident ~ 1 + (Repetition|SID) + (1|ImgName), data=WW2) 
```

```{r, echo=FALSE}
pacman::p_load(ordinal)

WW2 <- WW
WW2$Confident = factor(WW2$Confident, ordered = TRUE)
WW2$SID = factor(WW2$SID)

load("RData/clmm_NovSM_Exp3Word_Conf.RData")
```

```{r}
kable(anova(cm.full, cm.red1))
```

The effect of `item repetition` was not significant.

```{r, eval=FALSE, echo=FALSE}
( cm.full.emm <- emmeans(cm.full, pairwise ~ Repetition) )
plot(cm.full.emm, comparison = TRUE)
```

Below is the plot of estimated marginal means, which were extracted from the fitted CLMM. There was little difference between the `repetition` and `no repetition` conditions.

```{r}
pacman::p_load(RVAideMemoire)
temp <- emmeans(cm.full,~Repetition|cut,mode="linear.predictor")
temp <- rating.emmeans(temp)

colnames(temp)[1] <- "Condition"

ggplot(data = temp, aes(x = Rating, y = Prob, group = Condition)) +
  geom_line(aes(color = Condition), size = 1.2) +
  geom_point(aes(shape = Condition, color = Condition), 
             size = 4, fill = "white", stroke = 1.2) +
  scale_color_manual(values=c("#56B4E9", "#56B4E9")) +
  scale_shape_manual(name="Condition", values=c(24,21)) +
  labs(y = "Response Probability", x = "Rating") +
  expand_limits(y=0) +
  scale_y_continuous(limits = c(0, 0.5)) +
  scale_x_discrete(labels = c("1" = "1(Guessed)","4"="4(Sure)")) +
  theme_minimal() +
  theme(text = element_text(size=18))
```


## RT

Only RTs in correct trials were analyzed. Before analysis, we first removed RTs either shorter than 200ms or longer than 10s. Then, from the RT distribution of each condition, RTs beyond 3 SD from the mean were removed. 

```{r}
cWW <- WW %>% filter(Corr==1)

sWW <- cWW %>% filter(RT > 200 & RT < 10000) %>%
  group_by(Repetition) %>% 
  nest() %>% 
  mutate(lbound = map(data, ~mean(.$RT)-3*sd(.$RT)),
         ubound = map(data, ~mean(.$RT)+3*sd(.$RT))) %>% 
  unnest(lbound, ubound) %>% 
  unnest(data) %>% 
  mutate(Outlier = (RT < lbound)|(RT > ubound)) %>% 
  filter(Outlier == FALSE) %>%
  select(SID, Repetition, RT, ImgName)

100 - 100*nrow(sWW)/nrow(cWW)
```

This trimming procedure removed `r round(100 - 100*nrow(sWW)/nrow(cWW), digits = 2)`% of correct RTs.
  
Since the overall accuracy of source memory was not high, only small numbers of correct trials remained after trimming. None of participants had more than 20 valid trials. 

```{r}
sWW %>% group_by(SID, Repetition) %>% 
  summarise(NumTrial = length(RT)) %>%
  ungroup %>% 
  group_by(Repetition) %>%
  summarise(Avg = mean(NumTrial), 
            Med = median(NumTrial), 
            Min = min(NumTrial), 
            Max = max(NumTrial)) %>% 
  ungroup %>% 
  kable()
```

```{r}
h1 <- ggplot(cWW, aes(x=RT)) + geom_density()
h2 <- ggplot(sWW, aes(x=RT)) + geom_density() + labs(x = "Trimmed RT")

pacman::p_load(cowplot)
plot_grid(h1, h2)
```

The overall RT distribution was highly skewed even after trimming.

```{r}
apa_beeplot(data = sWW, 
            id="SID", dv="RT", factors="Repetition", 
            dispersion = conf_int,
            ylim = c(0, 5000),
            xlab = "Item Repetition",
            ylab = "Source Memory RT",
            las=1)

sWW %>% group_by(SID, Repetition) %>% 
  summarise(RT = mean(RT)) %>% 
  ungroup %>% 
  group_by(Repetition) %>%
  summarise(mean.RT = mean(RT), ci.RT = ci(RT)) %>% 
  ungroup %>% 
  kable()
```


### ANOVA

```{r, warning=FALSE}
ww.rt.aov <- aov_ez(id = "SID", dv = "RT", data = sWW, within = "Repetition")
anova(ww.rt.aov, es = "pes") %>% kable(digits = 4)
```

The difference was not significant. 


### GLMM
We built a linear mixed model that assumes an inverse Gaussian distribution and a linear relationship (identity link function) between the predictors and RT ([Lo & Andrews, 2015](https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full)). The current approach should be better than the conventional ANOVA in dealing with such small, unbalanced, and skewed data as the current RTs.
```{r, eval=FALSE}
require(parallel)
(nc <- detectCores())
cl <- makeCluster(rep("localhost", nc))

rt.full <- mixed(RT ~ Repetition + (Repetition|SID) + (1|ImgName),
               sWW, method = "LRT", cl = cl, 
               family=inverse.gaussian(link="identity"),
               control = glmerControl(optCtrl = list(maxfun = 1e6)))

stopCluster(cl)
```

```{r, echo=FALSE}
load('RData/afex_NovSM_Exp3Word_RT.RData')
```

```{r}
anova(rt.full) %>% kable()
```

The difference was not significant. 


# Session Info
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```



